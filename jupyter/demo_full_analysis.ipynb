{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Демонстрационный файл\n",
    "\n",
    "> Программа выполняет полный анализ текстового корпуса:\n",
    "> извлечение признаков, фильтрация текста, обработка слов,\n",
    "> составление вектора. Полученные данные сохраняются в БД.\n",
    "> Далее, выполняется классификация выбранного корпуса текстов\n",
    "> на основе полученных входных и выходных векторов.\n",
    "> Все параметры для настройки системы анализа текстовой \n",
    "> информации задаются файлом param.json\n",
    "\n",
    "\n",
    "**Подключение внешних библиотек:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from DbInteraction import DbInteraction\n",
    "from Param import Param\n",
    "from OpenTexts import OpenTexts\n",
    "from CorpusParser import CorpusParser\n",
    "from Dictionary import Dictionary\n",
    "from Vectorizer import Vectorizer\n",
    "from CorpusAnalyzer import CorpusAnalyzer\n",
    "import time\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Инициализация работы с БД:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.localtime()\n",
    "compilationTime = \"{0}.{1}.{2} {3}:{4}\".format(t.tm_year, t.tm_mon, \n",
    "                                               t.tm_mday, t.tm_hour, \n",
    "                                               t.tm_min)\n",
    "\n",
    "p = Param() #инициализация класса с параметрами работы\n",
    "db = DbInteraction() #иниц. класса для работы с БД\n",
    "db.initFullAnalysis(p.readDBCorpusPath()) #иниц. класса для работы с БД\n",
    "# отправка в него пути к БД\n",
    "corpusID = db.getCorpusID() #сохранение актуального ID, который\n",
    "# является индексом строки в БД\n",
    "db.updateCorpus('name', p.readName(), corpusID) #добавление начальной инфо\n",
    "# рмации о корпусе. данные считываются с параметров и отправляются\n",
    "db.updateCorpus('language', p.readLanguage(), corpusID) #\n",
    "db.updateCorpus('stemType', p.readStemType(), corpusID) #\n",
    "db.updateCorpus('stopWordsType', p.readStopWordsType(), corpusID) #\n",
    "db.updateCorpus('metric', p.readMetric(), corpusID) #\n",
    "db.updateCorpus('compilationTime', compilationTime, corpusID)\n",
    "\n",
    "\n",
    "db.addInfo()\n",
    "db.updateInfo('name', p.readName(), 1) #добавление начальной инфо\n",
    "# рмации о корпусе. данные считываются с параметров и отправляются\n",
    "db.updateInfo('language', p.readLanguage(), 1)\n",
    "db.updateInfo('stemType', p.readStemType(), 1)\n",
    "db.updateInfo('stopWordsType', p.readStopWordsType(), 1)\n",
    "db.updateInfo('metric', p.readMetric(), 1)\n",
    "db.updateInfo('corpus_ID', corpusID, 1)\n",
    "db.updateInfo('compilationTime', compilationTime, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Получение списка используемых текстов и отправка имени текста, \n",
    "имени темы и исходного текста в БД:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = CorpusAnalyzer() # аналайзер дополняет БД оставшимися данными\n",
    "\n",
    "op = OpenTexts(p.readDocCorpusPath()) # иниц. класса для работы с исходными\n",
    "# текстами\n",
    "op.searchFolder() # выбор метода для своего типа исходных данных \n",
    "# (поиск папок с файлами, файлов с текстами или другой)  \n",
    "# searchFolder, searchTxt, searchAlt\n",
    "# !!! мб перенести выбор метода в json параметры. а внутри класса пусть\n",
    "# сам определяет, какой метод надо использовать, на основе параметров \n",
    "while(op.hasNext()): # проверка на наличие следующего текста\n",
    "    tempData = op.getNext() # извлечение базовой информации из \n",
    "    # файла, сохранение в словаре\n",
    "    lastID = db.addTexts() # добавление новой строки в бд для \n",
    "    # информации по текстам и возврат её номера\n",
    "    db.updateTexts('name', tempData['name'], lastID) # обновление \n",
    "    # данных в соответствующей строке\n",
    "    db.updateTexts('topicName', tempData['topicName'], lastID) #\n",
    "    db.updateTexts('baseText', tempData['baseText'], lastID) #\n",
    "    \n",
    "    analyzer.addTopicName(tempData['topicName'])\n",
    "    db.updateTexts('topicNum', analyzer.getTopicNum(tempData['topicName']), lastID)\n",
    "    # <- аналайзер необходим, для получения списка используемых топиков.\n",
    "    # он запоминает имена топиков, присваивает им имена и, в данном месте,\n",
    "    # отправляет имена в БД, для отчётности и для дальнейшего формирования\n",
    "    # выходного вектора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обновление таблицы БД со списком тем для отчётности:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, val, i in zip(analyzer.getList().keys(), \n",
    "                     analyzer.getList().values(),\n",
    "                     range(analyzer.getNumOfTopics())):\n",
    "    db.addTopicList() # добавление новой строки в бд для списка топиков\n",
    "    db.updateTopicList('name', name, i+1)\n",
    "    db.updateTopicList('topicNum', val, i+1)\n",
    "    db.updateTopicList('numOfTexts', analyzer.getTopicCount(name), i+1)\n",
    "    # <- обновление информации в таблице со списком топиков\n",
    "    # общая информация, для отчетности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Фильтрация исходных текстов. Отправка обработанных текстов в БД:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = CorpusParser(language = p.readLanguage(), \n",
    "                      stemType = p.readStemType(),\n",
    "                      stopWordsType = p.readStopWordsType)\n",
    "tempText = ''\n",
    "for i in range(db.getTextsSize()):\n",
    "    tempText = db.getTextsData('baseText', i+1)[0][0]\n",
    "    tempText = parser.parsing(tempText)\n",
    "    db.updateTexts('formattedText', tempText, i+1)\n",
    "# <- выполняется полный проход по всем сырым текстам в бд\n",
    "# забираются сырые тексты, отправляются на очистку\n",
    "# возвращаются тексты после фильтрации и отправляются в БД обратно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Фомирование общего словаря для всех текстов и отправка\n",
    "словаря в БД:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dictionary()\n",
    "for i in range(db.getTextsSize()):\n",
    "    d.addData(db.getTextsData('formattedText', i+1)[0][0])\n",
    "    tempDict = d.getLastDictionary()\n",
    "    tempStr = json.dumps(tempDict)\n",
    "    tempStr = tempStr.replace('\"', '\"\"') \n",
    "    db.updateTexts('localDictionary', tempStr, i+1)\n",
    "    # <- добавление в БД локальных словарей в виде json строки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обновление общей информации, для отчётности:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if p.saveDictionary == True:\n",
    "    tempDict = d.getGlobalDictionary()\n",
    "    for key, val in tempDict.items():\n",
    "        lastID = db.addDictionary()\n",
    "        db.updateDictionary('word', key, lastID)\n",
    "        db.updateDictionary('value', val, lastID)\n",
    "        # <- добавление глобального словаря в бд, целиком\n",
    "        #!!! нужно пофиксить. работает слишком медленно\n",
    "   \n",
    "    \n",
    "  \n",
    "inputSize = d.getGlobalSize()\n",
    "outputSize = db.getTopicListSize()\n",
    "corpusSize = db.getTextsSize()\n",
    "\n",
    "db.updateCorpus('numOfTopics', outputSize, corpusID)\n",
    "db.updateInfo('numOfTopics', outputSize, 1)\n",
    "db.updateCorpus('numOfTexts', corpusSize, corpusID)\n",
    "db.updateInfo('numOfTexts', corpusSize, 1)\n",
    "db.updateCorpus('dictionarySize', inputSize, corpusID)\n",
    "db.updateInfo('dictionarySize', inputSize, 1)\n",
    "# <- обновление общей информации в БД (для отчетности)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Фомирование входных и выходных векторов к каждому тексту,\n",
    "отправка их в БД:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = Vectorizer()\n",
    "v.addGlobDict(d.getGlobalDictionary())\n",
    "for i in range(db.getTextsSize()):\n",
    "    tempStr = db.getTextsData('localDictionary', i+1)[0][0]\n",
    "    tempDict = json.loads(tempStr)\n",
    "    tempArray = v.getVecFromDict(tempDict)\n",
    "    tempStr = json.dumps(tempArray)\n",
    "    db.updateTexts('inputVector', tempStr, i+1)\n",
    "    # <- инициализация векторизатора, отправка глобального словаря в \n",
    "    # него, извлечение из бд локального словаря, преобразование \n",
    "    # его из json-строки в стандартный словарь отправка словаря \n",
    "    # в векторизатор, получение массива преобразование массива \n",
    "    # в json-строку и отправка обратно в бд\n",
    "    topicNum = db.getTextsData('topicNum', i+1)[0][0]\n",
    "    db.updateTexts('outputVector', \n",
    "                   json.dumps(v.numToOutputVec(topicNum, outputSize)),\n",
    "                   i+1)\n",
    "    # <- извлечение номера топика для формирования входного вектора\n",
    "    # и отправки этого вектора в БД"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Настройка объекта для последовательного извлечения \n",
    "входных и выходных данных:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = db.getConnectionData()\n",
    "ds = tf.data.Dataset.from_generator(\n",
    "    db.generator(corpusSize, db.getDataCorpusName(), 'inputVector', 'outputVector'),\n",
    "    output_types=(tf.float64, tf.float64),\n",
    "    output_shapes=(tf.TensorShape((inputSize, )), tf.TensorShape((outputSize, ))))\n",
    "    # <- использование генератора, который содержит весь набор данных и \n",
    "    # извлекает их, по необходимости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Разделение данных на обучающую и тестовую:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.shuffle(buffer_size=corpusSize,\n",
    "                reshuffle_each_iteration=True)\n",
    "trainSize = int(corpusSize*p.getTrainPercentage()/100)\n",
    "ds_train = ds.take(trainSize)\n",
    "ds_val = ds.skip(trainSize)\n",
    "ds = None\n",
    "ds_train = ds_train.batch(30)\n",
    "ds_val = ds_val.batch(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Формирование и подготовка нейронной сети:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(layers.Dense(inputSize, activation='relu'))\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(outputSize, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Начало процесса обучения сети:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(ds_train,\n",
    "                    epochs=100,\n",
    "                    validation_data=ds_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Построение графиков для отчета:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarize history for accuracy\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.plot(history.history['val_categorical_accuracy'])\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['val_categorical_accuracy', 'categorical_accuracy'], loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#summarize history for loss\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss', 'val_loss'], loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
